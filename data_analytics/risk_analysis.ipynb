{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1cdf1e5-c1e8-460d-bb5d-fd4d9d58b4fe",
   "metadata": {},
   "source": [
    "# Volatility Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976c385-e470-42a0-9de4-4501a5d3e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# os.getenv\n",
    "from dotenv import load_dotenv\n",
    "import hvplot.pandas\n",
    "import requests\n",
    "from utils import *\n",
    "from collect_contracts import *\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23730fda-949e-44d9-ae8c-dda21ca006c2",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0c2d6-0649-4aac-89dd-eb83fd828100",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "rarify_api_key = os.getenv(\"RARIFY_API_KEY\")\n",
    "display(type(rarify_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4120f6e9-d344-4bf0-a64d-900975e5c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_collections_baseurl = f\"https://api.rarify.tech/data/contracts?include=insights&sort=-unique_buyers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8537fdb-52d8-4f6b-9526-2eb430e01419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my analysis these two datapoints served to be problematic in skewing the data so I remove them here to avoid issues down the line\n",
    "# Top collections return is a dictionary holding the collection's contract id as the key and the network name being held as its value. \n",
    "\n",
    "top_collections_return = fetch_top_collections(top_collections_baseurl, rarify_api_key)\n",
    "\n",
    "del top_collections_return ['a310425046661c523d98344f7e9d66b32195365d']\n",
    "del top_collections_return ['495f947276749ce646f68ac8c248420045cb7b5e']\n",
    "del top_collections_return ['57f1887a8bf19b14fc0df6fd9b2acc9af147ea85']\n",
    "del top_collections_return ['c36442b4a4522e871399cd717abdd847ab11fe88']\n",
    "top_collections_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a417ee-2761-4a8f-b227-3ce0083e258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df = fetch_top_50_collections_data_api(top_collections_return, rarify_api_key)\n",
    "# collection_df = pd.DataFrame(collection_df)\n",
    "coll_df = collection_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f00f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02320df1-2360-4219-97b2-c5a5f5df5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with append_column_names(df, contract_ids)\n",
    "# append_column_names(coll_df, top_collections_return)\n",
    "\n",
    "cols = [\"avg_price\", \"max_price\", \"min_price\", \"trades\", \"unique_buyers\", \"volume\"]\n",
    "new_cols = []\n",
    "for key in top_collections_return.keys():\n",
    "    for c in cols:\n",
    "        new_cols.append(f\"{key}_{c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172258b9-bd96-48c9-b17a-d90354a202ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_collection_df = collection_df.copy()\n",
    "top_collection_df.columns = new_cols\n",
    "top_collection_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91914e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_price_df = find_avg_price_df(top_collection_df, top_collections_return)\n",
    "min_price_df = find_min_price_df(top_collection_df, top_collections_return)\n",
    "max_price_df = find_max_price_df(top_collection_df, top_collections_return)\n",
    "volume_df = find_volume_df(top_collection_df, top_collections_return)\n",
    "pct_chg_df = find_pct_change_df(top_collection_df, top_collections_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_df = volume_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4d7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev_df = find_std_dev_df(pct_chg_df, top_collections_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2540b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev_df = std_dev_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_price_df = avg_price_df.mean()\n",
    "min_price_df = min_price_df.mean()\n",
    "max_price_df = max_price_df.mean()\n",
    "pct_chg_df = pct_chg_df.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31542506",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_chg_df = pct_chg_df[0: len(top_collections_return)]\n",
    "pct_chg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cec817",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [avg_price_df, min_price_df, max_price_df, volume_df, pct_chg_df, std_dev_df]\n",
    "keys = ['avg_price', 'min_price', 'max_price', 'volume', 'pct_chg', 'std_dev']\n",
    "sum_df = pd.concat(df_list, axis=1, keys=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_df.to_csv('top_collections_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b62f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = sum_df[keys].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc87c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd171c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce296bc-8cc8-4673-bd0c-a328fd2d4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function appends pct_chg columns to the end of the current working dataframe\n",
    "pct_chg_pls_df = find_pct_change(top_collection_df, top_collections_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb95da34-1264-45ba-99b4-525a6a853b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the length of the Dataframe containing only the percent change values\n",
    "top_collections_pct_chg = pct_chg_pls_df.iloc[:, -(len(top_collections_return)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed79cef-95ce-4697-9280-5c170da712ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We append a new category here called the basket_pct_chg which is the mean of the pct_chg values of the top collections\n",
    "top_collections_pct_chg['top_collections_basket_pct_chg'] = top_collections_pct_chg[top_collections_pct_chg.columns].mean(axis=1)\n",
    "top_collections_pct_chg = top_collections_pct_chg.dropna()\n",
    "top_collections_pct_chg.head()\n",
    "top_collections_pct_chg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08958c-5d4a-42b8-a557-1406fa1d8885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find the average pct_chgs for the collections in our dataset\n",
    "pct_chgs = top_collections_pct_chg[top_collections_pct_chg.columns].mean()\n",
    "pct_chgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976568bd-9a09-49ec-8639-404be2a04d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we find the standard deviations of the pct changes for the top collections dataframe and we return only those standard deviation values from the dataframe\n",
    "def find_std_devs(df, contract_ids):\n",
    "    for col in df.columns:\n",
    "        df[f\"{col}_std_dev\"] = df[col].std()\n",
    "    return df[df.columns[-(len(contract_ids) + 1):]].mean()\n",
    "std_devs = find_std_devs(top_collections_pct_chg, top_collections_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f247a91f-108d-4148-ba2e-86cb32d02964",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0c6de-91c2-4358-8cc9-e9b1bc826151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation is an indication of the variability of the price changes of the data.  A collection with a higher standard deviation we would evaluate as more volatile.\n",
    "std_devs.hvplot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edebdd1e-4f2c-438d-b800-5643c7db4636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The betas for each collection are derived by comparing it to the whole set of data. \n",
    "# Ideally we would use a much broader index for this analysis which would take into account hundreds of NFT collections\n",
    "# For the time being, these figures can tell us how risky one collection is compared to the range of collections.\n",
    "# For fluctuations in the market, which collections are going to be able to withstand price changes in order to serve as safe and stable collateral\n",
    "\n",
    "betas = find_beta(top_collections_pct_chg, top_collections_return)\n",
    "betas_series = pd.Series(betas)\n",
    "betas_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da4b79-7609-41b6-ae5f-dcfdc89754a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a preprocessing step that I used to prototype a linear regression model that I ended up not using. This can be ignored for now. \n",
    "x_values = [0,1,2,3,4,5,6,7,8]\n",
    "analysis_list = [x_values, betas_series, std_devs, pct_chgs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae720d7-496e-4b47-9517-fd79a05df564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column names for our dataframe are renamed here to make them more reader friendly\n",
    "# The column names correspond to the statistics that we are analyzing\n",
    "# The row names correspond to a collection in our dataset excluding the 8th index which corresponds to the basket of collections\n",
    "index_y = {0: \"index\", 1: \"beta\", 2: \"std_dev\", 3: \"pct_chg\"}\n",
    "analysis_df = pd.DataFrame(analysis_list).transpose()\n",
    "analysis_df = analysis_df.rename(index_y, axis=1)\n",
    "# analysis_df = analysis_df.drop(8, axis=0)\n",
    "copy_analysis_df = analysis_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6c5f4-9340-417f-af2c-7fb028bac97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df['beta'].hvplot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6264bc-f7dd-429e-a4a2-a693573c5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.hvplot.scatter(x=\"std_dev\", y=\"beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91323220-2fe7-4cba-8820-5936c939b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each series in our dataframe is turned into a list to prepare it for a correlation coefficient function\n",
    "std_lst = list(std_devs[0:8].values)\n",
    "beta_lst = list(betas.values())\n",
    "pct_chg_lst = list(pct_chgs[0:8].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e9627",
   "metadata": {},
   "source": [
    "### For each statistic in our dataframe...\n",
    "We run a correlation function to find the correlation between that statistic and our Beta value. \n",
    "In doing so we can see how much each statistical measure (ie. avg_pct_chg, avg_std_dev) is correlated to our Beta value. \n",
    "This information may be obvious, (ie. standard deviation is used to calculate beta so it likely has a high correlation to beta, and standard deviation is a derivative of percent change, so percent change likely has a high correlation to Beta), however, I believe that it is important to see the correlational decay as we abstract to more fundamental observations. It may be beneficial to use a rolling percent change as a proxy for Beta and save time and computational resources, or it may be beneficial to take the extra step of deriving standard deviation. A correlational analysis will help shed light on that.\n",
    "\n",
    "It will also be useful going forward to include other variables in our correlational approach as it will all help paint a broader picture of what goes into determining the riskiness and volatility of an asset. In the following cells I also run a correlation function on average price and Beta. In that example I find that average price is weakly negatively correlated with a higher beta. This fact should not be surprising, and I expect this result to be reinforced with more data. To explain this, as a product has a higher value, it becomes more sought after in the market and is subject to greater market scrutiny, in addition to becoming more desirable. This helps to instantiate the true market value of the product with decreasing deviation in market price. A highly stable asset such as a piece of real estate, is highly stable because we can compare it to the value of thousands if not millions of others of the same class of asset in similar regions, or having similar specifications. We know that a million dollar house in San Francisco will not be worth 200,000 dollars tomorrow. Similarly, we can be more certain (evaluate as having less risk), that a Crypto Punk will not lose 75% of its value overnight. We may not be able to say the same as an emerging asset with relatively low trading volume.\n",
    "\n",
    "This analysis is all done with an unspoken (until now) assumption that Beta is the holy grail of risk analysis and volatility relative to the market. For the time being, I believe this to be the case however that notion could very easily be disrupted. Additional factors that we will invariably have to consider is how to determine the risk of an NFT asset versus an asset of another class. An analysis of this type will help to determine a baseline collateralization discount factor to apply to NFTs of almost any type. Perhaps NFTs as a whole are so vulnerable to volatility that we apply an additional 10% discount to their value when compared to something like a car. We also must keep in mind the duration of the loan being provided when making this decision. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_beta_corr = np.corrcoef(std_lst, beta_lst)\n",
    "std_beta_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b908971-fa6a-443a-bdd8-59c2dd3aa65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we see the decay of Beta correlation as we abstract to higher fundamental observations\n",
    "# Percent change of the average price of the collection correlates 86% to the Beta value\n",
    "pct_chg_beta_corr = np.corrcoef(pct_chg_lst, beta_lst)\n",
    "pct_chg_beta_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450221f0-c881-46da-bde4-7d383d6f6a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prices_df = find_avg_price(top_collection_df, top_collections_return)\n",
    "max_prices_df = find_max_price(top_collection_df, top_collections_return)\n",
    "min_prices_df = find_min_price(top_collection_df, top_collections_return)\n",
    "volume_df = find_volume(top_collection_df, top_collections_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec864cc-4875-4ba8-9691-451afd9b01d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prices = avg_prices_df.iloc[0]\n",
    "avg_prices_lst = list(avg_prices[0:8].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9804a4d0-6711-4917-87ac-301e1cfcc915",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prices = max_prices_df.iloc[0]\n",
    "max_prices_lst = list(max_prices[0:8].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55370be3-2f61-4bf9-b449-b56125bb005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_prices = min_prices_df.iloc[0]\n",
    "min_prices_lst = list(min_prices[0:8].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa7a86-28d0-4a1c-ad51-e1e37a87e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_avg = volume_df.iloc[0]\n",
    "volume_lst = list(vol_avg[0:8].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511a03a-3295-438b-b5b7-ee733eb3565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, average prices weakly, negatively correlates to beta\n",
    "avg_prices_beta_corr = np.corrcoef(avg_prices_lst, beta_lst)\n",
    "avg_prices_beta_corr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a283dc85-fc9e-4510-8d56-6dea3a354784",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prices_beta_corr = np.corrcoef(max_prices_lst, beta_lst)\n",
    "max_prices_beta_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35dc5ee-dc9f-46db-aff9-d9aefea63e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_prices_beta_corr = np.corrcoef(min_prices_lst, beta_lst)\n",
    "min_prices_beta_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab75d5-6bbc-4a00-bd5a-d8c2230559bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_beta_corr = np.corrcoef(volume_lst, beta_lst)\n",
    "vol_beta_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859ce45",
   "metadata": {},
   "source": [
    "### Results\n",
    "The results of the average prices correlation to beta reveal that average price could be used to weakly indicate the stability of a collection but it certainly isn't a silver bullet method. This is promising because it opens the door for smaller collections to be using used as a stable enough asset for collateralization. Our initial hypothesis is generally true that we can have more faith in the top collections to perform well as collateral, but singularly viewing top collections as the only viable asset is an error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53fb114-a5d5-4bee-b3b2-e337dbc7757b",
   "metadata": {},
   "source": [
    "## Future Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91827be-52ae-40f8-a9df-e0606d5de4ff",
   "metadata": {},
   "source": [
    "* community size (twitter followers, etc.)\n",
    "* time in the market\n",
    "* unique buyers\n",
    "* whale holders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982ee14-95d2-41cb-b12c-e8c34bd0b563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d2d6760d3733bf9fc8b7a105753e52665e6329ccd4e8539ab85fef63649f337"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
