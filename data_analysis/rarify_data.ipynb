{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Collection Analysis and Monte Carlo Projection for Top NFT Collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# os.getenv\n",
    "from dotenv import load_dotenv\n",
    "import hvplot.pandas\n",
    "import requests\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "rarify_api_key = os.getenv(\"RARIFY_API_KEY\")\n",
    "display(type(rarify_api_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching from the Rarify API\n",
    "* We get the data for our nft collections from the rarify API\n",
    "* We are targeting the collections data endpoint which is the following: \"https://api.rarify.tech/data/contracts/{network_id}:{contract_id}/insights/90d\"\n",
    "* We supply the network_id as the blockchain which is Ethereum in our case\n",
    "* In the first instance we will target the crypto punks collection by supplying it's contract_id to check that our authentication and fetch method works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "network_id = \"ethereum\"\n",
    "# Crypto Punks\n",
    "contract_id = \"b47e3cd837ddf8e4c57f05d70ab865de6e193bbb\"\n",
    "\n",
    "collections_baseurl = f\"https://api.rarify.tech/data/contracts/{network_id}:{contract_id}/insights/90d\"\n",
    "\n",
    "# Use the following code to target a specific token in the collection\n",
    "token_id = 9620\n",
    "token_baseurl = f\"https://api.rarify.tech/data/tokens/{network_id}:{contract_id}:{token_id}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_rarify_data(url, key):\n",
    "    \"\"\"\n",
    "    The following function is our base fetch for the collection data using our authorization key stored in the environment\n",
    "    variables as well as the url that we supply to the function\n",
    "    The url must be supplied with a valid network_id, contract_id, and token_id\n",
    "    The function returns the sale_history_data for our targeted collection at the 'history' endpoint\n",
    "    \"\"\"\n",
    "    sale_history_data = requests.get(\n",
    "        url,\n",
    "        headers={\"Authorization\": f\"Bearer {key}\"}\n",
    "    ).json()\n",
    "    return sale_history_data['included'][1]['attributes']['history']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Data Object\n",
    "* We instantiate the punks_return object as a fetch at our api endpoint\n",
    "* We turn the return into a DataFrame\n",
    "* We set the 'time' column to a datetime type object\n",
    "* We set the index of our data to the 'time' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punks_return = fetch_rarify_data(collections_baseurl, rarify_api_key)\n",
    "punks_df = pd.DataFrame(punks_return)\n",
    "punks_df['time'] = pd.to_datetime(punks_df['time'], infer_datetime_format=True)\n",
    "punks_df = punks_df.set_index('time')\n",
    "\n",
    "punks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Conversion\n",
    "* Our numeric data is returned as strings so we must process it\n",
    "* We use a dict to convert the types of each numeric column to a float type using the df.astype() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dict = {'avg_price': float,\n",
    "                'max_price': float,\n",
    "                'min_price': float,\n",
    "                'trades': float,\n",
    "                'unique_buyers': float,\n",
    "                'volume': float,\n",
    "               }  \n",
    "  \n",
    "punks_df = punks_df.astype(convert_dict)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Factoring\n",
    "* We multiply the numeric data that is given to us in gwei by a factor of 10^-18 to convert it to eth prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punks_df[['avg_price', 'max_price', 'min_price', 'volume']] = punks_df[['avg_price', 'max_price', 'min_price', 'volume']] * 10**-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms\n",
    "* Collateral Discount Factor: A percentage that the collateral's value must be discounted in order to ensure a safe return for the lender should the borrower default on his loan. This value differs by the type of asset and is somewhat arbitrary but is based largely on expert appraisal (ie. a car used as collateral may grant the borrower a loan of 50% of the appraised value of the car. The collateral discount factor would be 50%). \n",
    "\n",
    "* Collateral Coverage Ratio (CCR): The discounted value of the collateralized asset over the value of a loan that a borrower is looking to receive. A higher CCR (over 1.0) indicates sufficient collateral which will cover the value of the loan at the discounted value of the collateral. (ie. John would like a loan of 10,000 and puts his car, worth 25,000 up as collateral. If a 50% collateral discount factor is applied to John's car, the resulting CCR is 1.225. This would be a safe loan for the lender because he could easily cover his costs, and profit, should the borrower default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation for the minimum price of the Punks collection\n",
    "punks_df['min_price'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the minimum, average and maximum price for the collection along time\n",
    "punks_df[['min_price', 'avg_price', 'max_price']].hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plot is hard to read so we will just plot the average price along with the mean of the average price\n",
    "punks_df['mean_avg'] = punks_df['avg_price'].mean()\n",
    "punks_df[['avg_price', 'mean_avg']].hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take a look at the min_price\n",
    "punks_df[\"mean_min\"] = punks_df['min_price'].mean()\n",
    "punks_df[['mean_min', 'min_price']].hvplot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punks_df['min_price'].rolling(window=10).std().hvplot(title=\"min_price standard deviation rolling window=10 days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot it looks like there is little if any trend in the data from the start to the end of the previous 90 days. This may actually be a good signal as it shows that items from the collection may provide stable collateral. For the stability and value of this collection, we would apply a relatively low collateral discount factor for this asset based on its performance and its projected performance overtime. However, the standard deviation of the asset is quite high, this is largely due to the illiquity of NFTs and the relatively few sales that occur on a given day. NFTs in general should be granted a relatively high collateral discount factor compared to other asset classses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing a Series of Collections\n",
    "* First we aggregate a series of reputable collections from opensea and their contract addresses\n",
    "* I selected the following collections, but any number of collections would work for analysis:\n",
    "* *Bored Ape Yacht Club*, *Crypto Punks*, *Clone X*, *Doodles*, *NeoTokyo*, and *Mfers*\n",
    "* These are all some of the highest performers on OpenSea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of collection addresses: \n",
    "# bape: 0xBC4CA0EdA7647A8aB7C2061c2E118A18a936f13D\n",
    "# punks: b47e3cd837ddf8e4c57f05d70ab865de6e193bbb\n",
    "# clone x: 0x49cF6f5d44E70224e2E23fDcdd2C053F30aDA28B\n",
    "# doodles: 0x8a90CAb2b38dba80c64b7734e58Ee1dB38B8992e\n",
    "# neotokyo: 0xb668beB1Fa440F6cF2Da0399f8C28caB993Bdd65\n",
    "# mfers: 0x79FCDEF22feeD20eDDacbB2587640e45491b757f\n",
    "\n",
    "def get_collections_data(contract_ids: dict, rarify_api_key: str):\n",
    "    \"\"\"\n",
    "    *The following function is quite messy and I will clean it up at a later time but it will work for now.*\n",
    "    This function aggregates the data from a selection of NFT collections into a double-layered DataFrame which can be used to run a Monte Carlo simulation\n",
    "    \n",
    "    :param contract_ids: (type: dict) Houses the contract addresses and the collection names\n",
    "    :param rarify_api_key: (type: str) Your authentication key from the rarify API\n",
    "    \n",
    "    The function iterates through the dictionary of addresses that you supply to it and makes an API call for each address.\n",
    "    It then takes the relevant data and turns it into a DataFrame object.\n",
    "    We then preprocess the data like we did before, formatting and setting the index as the 'time' column,\n",
    "    and converting the string numbers to integers using the df.astype() method. We also convert the prices to eth from gwei using a factor. \n",
    "    We then append the most recently constructed dataframe to the list that we instantiated at the top of the function\n",
    "    \n",
    "    :returns: A concatenation of all the DataFrames that are present in the DataFrame list that we constructed.\n",
    "\n",
    "\n",
    "    *There is obviously much more elegant way to conduct this process so let me know if you have a cleaner way of doing this*\n",
    "\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    network_id = \"ethereum\"\n",
    "    convert_dict = {\n",
    "                    'avg_price': float,\n",
    "                    'max_price': float,\n",
    "                    'min_price': float,\n",
    "                    'trades': float,\n",
    "                    'unique_buyers': float,\n",
    "                    'volume': float,\n",
    "                   }  \n",
    "    for address in contract_ids.values():\n",
    "        contract_id = address\n",
    "        collections_baseurl = f\"https://api.rarify.tech/data/contracts/{network_id}:{contract_id}/insights/90d\"\n",
    "        curr_df = pd.DataFrame(fetch_rarify_data(collections_baseurl, rarify_api_key))\n",
    "        curr_df['time'] = pd.to_datetime(curr_df['time'], infer_datetime_format=True)\n",
    "        curr_df = curr_df.set_index('time')\n",
    "        curr_df = curr_df.astype(convert_dict)\n",
    "        curr_df[['avg_price', 'max_price', 'min_price', 'volume']] = curr_df[['avg_price', 'max_price', 'min_price', 'volume']] * 10**-18\n",
    "        df_list.append(curr_df)\n",
    "    sum_df = pd.concat(df_list, axis=1, keys=contract_ids.keys())\n",
    "    return sum_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I might use these as functions inside the main function at some point but I will have to restructure the framework\n",
    "# So for now I will set these functions aside here\n",
    "def set_time_index(df):\n",
    "    df['time'] = pd.to_datetime(df['time'], infer_datetime_format=True)\n",
    "    df = df.set_index('time')\n",
    "    return df\n",
    "\n",
    "def convert_str_int(df):\n",
    "    convert_dict = {'avg_price': float,\n",
    "                'max_price': float,\n",
    "                'min_price': float,\n",
    "                'trades': float,\n",
    "                'unique_buyers': float,\n",
    "                'volume': float,\n",
    "               }  \n",
    "    df = df.astype(convert_dict) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The collections that we will take a look at with their contract addresses\n",
    "contract_ids = {\n",
    "                \"bape\": \"0xBC4CA0EdA7647A8aB7C2061c2E118A18a936f13D\", \n",
    "                \"punks\": \"b47e3cd837ddf8e4c57f05d70ab865de6e193bbb\", \n",
    "                \"clonex\": \"0x49cF6f5d44E70224e2E23fDcdd2C053F30aDA28B\",\n",
    "                \"doodles\": \"0x8a90CAb2b38dba80c64b7734e58Ee1dB38B8992e\",\n",
    "                \"neotokyo\": \"0xb668beB1Fa440F6cF2Da0399f8C28caB993Bdd65\",\n",
    "                \"mfers\": \"0x79FCDEF22feeD20eDDacbB2587640e45491b757f\",\n",
    "}\n",
    "\n",
    "# Store the resulting concatenated DataFrame in a sum_df object\n",
    "\n",
    "sum_df = get_collections_data(contract_ids, rarify_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More preprocessing\n",
    "* In order to do anything very meaningful with the data it is helpful to rename the columns\n",
    "* We will rename the columns with the prefix \"key_\" added to each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"avg_price\", \"max_price\", \"min_price\", \"trades\", \"unique_buyers\", \"volume\"]\n",
    "new_cols = []\n",
    "for key in contract_ids.keys():\n",
    "    for c in cols:\n",
    "        new_cols.append(f\"{key}_{c}\")\n",
    "        \n",
    "new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I create a new object of the sum_df with the new columns applied to it. \n",
    "I want to leave sum_df the way it is because I will use it for the Monte Carlo simulation later.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df = sum_df.copy()\n",
    "collection_df.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the rolling 30 days standard deviation for each of the collections average price normalized by the average price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_30_std = collection_df[[\"bape_avg_price\", \"clonex_avg_price\", \"punks_avg_price\", \"neotokyo_avg_price\", \"doodles_avg_price\", \"mfers_avg_price\"]].rolling(window=30).std() / collection_df[[\"bape_avg_price\", \"clonex_avg_price\", \"punks_avg_price\", \"neotokyo_avg_price\", \"doodles_avg_price\", \"mfers_avg_price\"]] \n",
    "rolling_30_std.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the doodles and mfers have the highest normalized standard deviations and clonex has the lowest. If we were evaluating a loan based solely on std we would apply the greatest collateral discount factor to mfers and doodles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Monte Carlo Projection for our selected collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This projection takes the previous 90 days of data and predicts the next 30 days of returns if we held a basket of these NFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MCForecastTools import MCSimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation set to iterate 100 times over the next 30 trading days\n",
    "# we leave the default weights which will be 1/6 per collection\n",
    "\n",
    "# in the MCForecastTools.py file the 'close' column was changed to 'avg_price' to fit our data\n",
    "sim = MCSimulation(sum_df, num_simulation=100, num_trading_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.portfolio_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(f\"bape: {sim.portfolio_data['bape']['daily_return'].mean()}\")\n",
    "display(f\"punks: {sim.portfolio_data['punks']['daily_return'].mean()}\")\n",
    "display(f\"clonex: {sim.portfolio_data['clonex']['daily_return'].mean()}\")\n",
    "display(f\"doodles: {sim.portfolio_data['doodles']['daily_return'].mean()}\")\n",
    "display(f\"neotokyo: {sim.portfolio_data['neotokyo']['daily_return'].mean()}\")\n",
    "display(f\"mfers: {sim.portfolio_data['mfers']['daily_return'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these collections have all performed strongly over the last ninety days, each with a positive average daily return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_return = sim.calc_cumulative_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_return.hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_return.to_csv('mc_cum_return.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('mc_cum_return.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_return.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_return.mean().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the forecasted returns for this basket of NFTs it would be a good selection candidate for collateralization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta Analysis For NFT versus Basket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What we want to do here is find the relative risk for each asset in the basket versus the basket as a whole.\n",
    "* For instance, we will compare Crypto Punks, etc. to the 6 NFT collection that we selected using Beta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df = collection_df.drop(\"bape_pct_chg\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pct_change(df, contract_ids):\n",
    "    coll_names = []\n",
    "    counter = 0\n",
    "    for k in contract_ids.keys():\n",
    "        coll_names.append(k)\n",
    "    for col in df.columns:\n",
    "        if \"avg_price\" in col:\n",
    "            df[f\"{coll_names[counter]}_pct_chg\"] = df[col].pct_change()\n",
    "            counter += 1\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_pct_change(collection_df, contract_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basket_pct_chg(df, contract_ids):\n",
    "    coll_names = []\n",
    "    pct_chg_lst = []\n",
    "    for k in contract_ids.keys():\n",
    "        coll_names.append(k)\n",
    "    for col in df.columns:\n",
    "        if \"pct_chg\" in col:\n",
    "            pct_chg_lst.append(col)\n",
    "    basket_df = df[pct_chg_lst]\n",
    "    return basket_df.dropna()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_df = basket_pct_chg(collection_df, contract_ids)\n",
    "basket_df[\"basket_pct_chg\"] = basket_df[basket_df.columns].mean(axis=1)\n",
    "\n",
    "basket_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bape_beta = basket_df[\"bape_pct_chg\"].cov(basket_df[\"basket_pct_chg\"]) / basket_df[\"basket_pct_chg\"].var()\n",
    "punks_beta = basket_df[\"punks_pct_chg\"].cov(basket_df[\"basket_pct_chg\"]) / basket_df[\"basket_pct_chg\"].var()\n",
    "neo_beta = basket_df[\"neotokyo_pct_chg\"].cov(basket_df[\"basket_pct_chg\"]) / basket_df[\"basket_pct_chg\"].var()\n",
    "clonex_beta = basket_df[\"clonex_pct_chg\"].cov(basket_df[\"basket_pct_chg\"]) / basket_df[\"basket_pct_chg\"].var()\n",
    "doodles_beta = basket_df[\"doodles_pct_chg\"].cov(basket_df[\"basket_pct_chg\"]) / basket_df[\"basket_pct_chg\"].var()\n",
    "mfers_beta = basket_df[\"mfers_pct_chg\"].cov(basket_df[\"basket_pct_chg\"]) / basket_df[\"basket_pct_chg\"].var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_list = [bape_beta, punks_beta, neo_beta, clonex_beta, doodles_beta, mfers_beta]\n",
    "betas = pd.DataFrame([bape_beta, punks_beta, neo_beta, clonex_beta, doodles_beta, mfers_beta], index=contract_ids.keys())\n",
    "betas.hvplot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the beta analysis data, we would be more inclined to use the collections with the lowest beta values as collateral because they indicate greater security against the market. In this case, Crypto Punks and Doodles serve as the best candidates for collateralization and borrowers would be rewarded with a potentially lower collateral discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epilogue\n",
    "## Foreshadowing a Collateral Discount Curve Based on Beta Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we want to appraise how much of a collateral discount factor to apply to a token/collection based on its beta value. In terms of Beta values, a higher one should correspond to a higher discount factor applied to the asset. If an asset is more risky, we can only safely provide a smaller loan. For collateral assets the holy grail is high stability and is even better if it is highly stable appreciation (ie. real estate). If a borrower defaults on a loan it is reassuring to know that the asset used as collateral has either the same value or a higher value than when we received it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_factor(betas):\n",
    "    discount_factors = []\n",
    "    for beta in betas:\n",
    "        disc_factor = 1 - (1/(beta + 1.5)) + .1697\n",
    "        discount_factors.append(disc_factor)\n",
    "    return discount_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factors_list = discount_factor(betas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factors = pd.DataFrame(discount_factors_list, index=contract_ids.keys())\n",
    "discount_factors.hvplot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Say\n",
    "I'm a user with a Doodles NFT that is worth 15eth and I am looking for a loan. Based on the discount curve what kind of a loan could I expect to receive for my NFT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_loan_value(collection: str, value: float, discount_factors):\n",
    "    loan_value = value - value * discount_factors[0][collection]\n",
    "    return loan_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_value = find_loan_value(\"doodles\", 15, discount_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The loan calculator has determined that you are eligible to receive {loan_value: .2f}eth for your NFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "be924676cdfa25a697f162e38e58c9c2d4ca83b5aad6fd188aa72795ee63fc3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
